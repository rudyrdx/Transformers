{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rvn/.envs/torch-2.2.0-cu-12.1/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding to ensure reproducibility\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset is very easy using the `datasets` package! The command below will download (if required in the *data/classification* directory) and load the IMDB dataset in a variable named `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"zapsdcn/imdb\", cache_dir=\"../data/classification/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded `dataset` is already split into training, validation and testing sets. Looking at the outputs, we can see the number of examples (`num_rows`) in each split and three features named `id`, `text` and `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 20000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train'] \n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at one such training example, we can see that \n",
    "- `id` contains information related to some index. Don't bother too much about this as this is not relevant for the application at hand.\n",
    "- `text` is where we have the actual review!\n",
    "- `label` denotes whether it's a good (if 1) or bad (if 0) review! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_10348',\n",
       " 'text': 'This movie is very good. The screenplay is enchanting. But Meryl Streep is most impressive. Her performance is excellent. She brings me to go into the heart of her role.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers only understand numbers. So we have to find a way to modify such that we can represent each word as a number. But first we have to split sentences into individual words. This is called Tokenization and is done using a tokenizer! \n",
    "\n",
    "I'm using a simple tokenizer provided by `torchtext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a tokenizer splits a text into words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(train_dataset[-3][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this for all of the training, validation and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, max_length):\n",
    "    tokens = tokenizer(example[\"text\"])[:max_length]\n",
    "    return {\"tokens\": tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncating the max length of a review because some of them ramble on for quite some time!\n",
    "max_length = 512\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "validation_dataset = validation_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a volcabulary out of these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only selecting words at occur at least 5 times in the whole dataset\n",
    "min_freq = 5\n",
    "\n",
    "# Special tokens for unknown words and paddings (these will come in later)\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_dataset[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab), vocab.get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that words that are not in the vocabulary gets mapped to `\"<unk>\"` and assiging a token for padding as well (we'll cover padding later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "\n",
    "vocab.set_default_index(unk_index)\n",
    "\n",
    "vocab[\"some_token_that_is_not_in_vocab\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with this we can easily map any review to a sequence of numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.lookup_indices(train_dataset[-3][\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this in a more refined way for all of the datasets and store these number in the `id` feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(example, vocab):\n",
    "    id = vocab.lookup_indices(example[\"tokens\"])\n",
    "    return {\"id\": id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})\n",
    "validation_dataset = validation_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})\n",
    "test_dataset = test_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to torch tensors to be loaded into Transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])\n",
    "validation_dataset = validation_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])\n",
    "test_dataset = test_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pad the sequences to make them of equal lengths and then load the dataset into pytorch data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_ids = [i[\"id\"] for i in batch]\n",
    "        batch_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            batch_ids, padding_value=pad_index, batch_first=True\n",
    "        )\n",
    "        batch_label = [i[\"label\"] for i in batch]\n",
    "        batch_label = torch.stack(batch_label)\n",
    "        batch = {\"id\": batch_ids, \"label\": batch_label}\n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "train_data_loader = get_data_loader(train_dataset, batch_size, pad_index, shuffle=True)\n",
    "validation_data_loader = get_data_loader(validation_dataset, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_dataset, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of pytorch data loaders as an iterable over which you can loop and get batches one by one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data_loader:\n",
    "    print(batch[\"id\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7333, -0.2087, -0.1684,  0.8662, -0.7795,  1.2341, -0.6230],\n",
       "        [-1.3696, -0.8951,  0.8114, -1.0988, -1.2615,  0.4062, -0.3548],\n",
       "        [-0.4408,  0.2652, -1.0598,  1.9366,  0.5431, -1.2966, -0.1174],\n",
       "        [-0.8789, -0.2537, -0.0356,  0.8743,  0.3683,  1.5304,  1.6146],\n",
       "        [ 0.9612,  0.1940, -0.7919, -1.4849, -0.6838,  0.1440,  1.0798]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_review = train_dataset[-3][\"id\"][:5]\n",
    "token_embedding = torch.nn.Embedding(embedding_dim=7, num_embeddings=len(vocab))\n",
    "emb_token = token_embedding(ex_review)\n",
    "emb_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../anim/ex_review_emb.npy\", emb_token.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_token_ = np.load(\"../anim/ex_review_emb.npy\")\n",
    "emb_token_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.73, -0.21, -0.17,  0.87, -0.78,  1.23, -0.62],\n",
       "       [-1.37, -0.9 ,  0.81, -1.1 , -1.26,  0.41, -0.35],\n",
       "       [-0.44,  0.27, -1.06,  1.94,  0.54, -1.3 , -0.12],\n",
       "       [-0.88, -0.25, -0.04,  0.87,  0.37,  1.53,  1.61],\n",
       "       [ 0.96,  0.19, -0.79, -1.48, -0.68,  0.14,  1.08]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(emb_token_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2.2.0-cu-12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
