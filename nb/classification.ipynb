{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rvn/.envs/torch-2.2.0-cu-12.1/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding to ensure reproducibility\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset is very easy using the `datasets` package! The command below will download (if required in the *data/classification* directory) and load the IMDB dataset in a variable named `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"zapsdcn/imdb\", cache_dir=\"../data/classification/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded `dataset` is already split into training, validation and testing sets. Looking at the outputs, we can see the number of examples (`num_rows`) in each split and three features named `id`, `text` and `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 20000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train'] \n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at one such training example, we can see that \n",
    "- `id` contains information related to some index. Don't bother too much about this as this is not relevant for the application at hand.\n",
    "- `text` is where we have the actual review!\n",
    "- `label` denotes whether it's a good (if 1) or bad (if 0) review! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_10348',\n",
       " 'text': 'This movie is very good. The screenplay is enchanting. But Meryl Streep is most impressive. Her performance is excellent. She brings me to go into the heart of her role.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers only understand numbers. So we have to find a way to modify such that we can represent each word as a number. But first we have to split sentences into individual words. This is called Tokenization and is done using a tokenizer! \n",
    "\n",
    "I'm using a simple tokenizer provided by `torchtext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a tokenizer splits a text into words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(train_dataset[-3][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this for all of the training, validation and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, max_length):\n",
    "    tokens = tokenizer(example[\"text\"])[:max_length]\n",
    "    return {\"tokens\": tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncating the max length of a review because some of them ramble on for quite some time!\n",
    "max_length = 512\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "validation_dataset = validation_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a volcabulary out of these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only selecting words at occur at least 5 times in the whole dataset\n",
    "min_freq = 5\n",
    "\n",
    "# Special tokens for unknown words and paddings (these will come in later)\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_dataset[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab), vocab.get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that words that are not in the vocabulary gets mapped to `\"<unk>\"` and assiging a token for padding as well (we'll cover padding later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "\n",
    "vocab.set_default_index(unk_index)\n",
    "\n",
    "vocab[\"some_token_that_is_not_in_vocab\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with this we can easily map any review to a sequence of numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.lookup_indices(train_dataset[-3][\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this in a more refined way for all of the datasets and store these number in the `id` feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(example, vocab):\n",
    "    id = vocab.lookup_indices(example[\"tokens\"])\n",
    "    return {\"id\": id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})\n",
    "validation_dataset = validation_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})\n",
    "test_dataset = test_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to torch tensors to be loaded into Transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])\n",
    "validation_dataset = validation_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])\n",
    "test_dataset = test_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pad the sequences to make them of equal lengths and then load the dataset into pytorch data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_ids = [i[\"id\"] for i in batch]\n",
    "        batch_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            batch_ids, padding_value=pad_index, batch_first=True\n",
    "        )\n",
    "        batch_label = [i[\"label\"] for i in batch]\n",
    "        batch_label = torch.stack(batch_label)\n",
    "        batch = {\"id\": batch_ids, \"label\": batch_label}\n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 512\n",
    "\n",
    "train_data_loader = get_data_loader(train_dataset, batch_size, pad_index, shuffle=True)\n",
    "validation_data_loader = get_data_loader(validation_dataset, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_dataset, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of pytorch data loaders as an iterable over which you can loop and get batches one by one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data_loader:\n",
    "    print(batch[\"id\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2282,  0.2800,  0.0732,  1.1133,  0.2823,  0.4342,  0.4569],\n",
       "        [-0.0899,  0.7298, -1.8453, -0.1021, -1.0335, -0.3126,  0.2458],\n",
       "        [ 1.7067,  2.3804, -1.0670,  1.1149, -0.1407,  0.8058,  0.3276],\n",
       "        [ 0.9929, -0.2065, -0.2448, -0.2793, -0.2769,  0.7489, -0.6435],\n",
       "        [-0.4781,  1.3892, -0.5023,  1.6797, -1.0240, -0.5753, -1.4325]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_review = train_dataset[-3][\"id\"][:5]\n",
    "token_embedding = torch.nn.Embedding(embedding_dim=7, num_embeddings=len(vocab))\n",
    "emb_token = token_embedding(ex_review)\n",
    "emb_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.7333, -0.2087, -0.1684,  0.8662, -0.7795,  1.2341, -0.6230],\n",
       "          [-1.3696, -0.8951,  0.8114, -1.0988, -1.2615,  0.4062, -0.3548],\n",
       "          [-0.4408,  0.2652, -1.0598,  1.9366,  0.5431, -1.2966, -0.1174],\n",
       "          [-0.8789, -0.2537, -0.0356,  0.8743,  0.3683,  1.5304,  1.6146],\n",
       "          [ 0.9612,  0.1940, -0.7919, -1.4849, -0.6838,  0.1440,  1.0798]]]),\n",
       " torch.Size([1, 5, 7]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load(\"../anim/ex_review_emb.npy\")\n",
    "X = torch.tensor(X)\n",
    "X = X.unsqueeze(dim=0)\n",
    "X, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention\n",
    "def self_attention(X):\n",
    "    # Weight Matrix\n",
    "    W = torch.bmm(X, X.transpose(1, 2))\n",
    "    W = W / (7**(1/2)) # Scaling for stability\n",
    "    W = F.softmax(W, dim=-1)\n",
    "\n",
    "    # Output Matrix\n",
    "    y = torch.bmm(W, X)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.2696, -0.1809, -0.2182,  0.6464, -0.6488,  1.0064, -0.2963],\n",
       "          [-1.0309, -0.7250,  0.5664, -0.8579, -1.0547,  0.4712, -0.1181],\n",
       "          [-0.3646,  0.1967, -0.9301,  1.7236,  0.4302, -0.9713, -0.0155],\n",
       "          [-0.6338, -0.2349, -0.0945,  0.6974,  0.1573,  1.1944,  1.2254],\n",
       "          [ 0.6570,  0.0204, -0.5302, -0.9239, -0.6334,  0.3498,  0.7731]]]),\n",
       " torch.Size([1, 5, 7]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = self_attention(X)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../anim/ex_review_out.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 0, 1, 3, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.randperm(X.shape[1])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.7333, -0.2087, -0.1684,  0.8662, -0.7795,  1.2341, -0.6230],\n",
       "          [-1.3696, -0.8951,  0.8114, -1.0988, -1.2615,  0.4062, -0.3548],\n",
       "          [-0.4408,  0.2652, -1.0598,  1.9366,  0.5431, -1.2966, -0.1174],\n",
       "          [-0.8789, -0.2537, -0.0356,  0.8743,  0.3683,  1.5304,  1.6146],\n",
       "          [ 0.9612,  0.1940, -0.7919, -1.4849, -0.6838,  0.1440,  1.0798]]]),\n",
       " tensor([[ 0.9612,  0.1940, -0.7919, -1.4849, -0.6838,  0.1440,  1.0798],\n",
       "         [ 1.7333, -0.2087, -0.1684,  0.8662, -0.7795,  1.2341, -0.6230],\n",
       "         [-1.3696, -0.8951,  0.8114, -1.0988, -1.2615,  0.4062, -0.3548],\n",
       "         [-0.8789, -0.2537, -0.0356,  0.8743,  0.3683,  1.5304,  1.6146],\n",
       "         [-0.4408,  0.2652, -1.0598,  1.9366,  0.5431, -1.2966, -0.1174]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = self_attention(X[0][idx])\n",
    "y_, y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2.2.0-cu-12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
